{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH=\"/kaggle/input/emotions-dataset-for-nlp/train.txt\"\n",
    "TEST_DATA_PATH=\"/kaggle/input/emotions-dataset-for-nlp/test.txt\"\n",
    "VAL_DATA_PATH=\"/kaggle/input/emotions-dataset-for-nlp/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(columns=['sentence', 'label'])\n",
    "test_data = pd.DataFrame(columns=['sentence', 'label'])\n",
    "val_data = pd.DataFrame(columns=['sentence', 'label'])\n",
    "\n",
    "with open(TRAIN_DATA_PATH, 'r') as file:\n",
    "    for _, line in enumerate(file):\n",
    "        train_data = pd.concat([\n",
    "            pd.DataFrame(\n",
    "                [line.replace('\\n', '').split(';')], columns=train_data.columns\n",
    "            ),\n",
    "            train_data\n",
    "        ], ignore_index=True)\n",
    "\n",
    "with open(TEST_DATA_PATH, 'r') as file:\n",
    "    for _, line in enumerate(file):\n",
    "        test_data = pd.concat([\n",
    "            pd.DataFrame(\n",
    "                [line.replace('\\n', '').split(';')], columns=test_data.columns\n",
    "            ),\n",
    "            test_data\n",
    "        ], ignore_index=True)\n",
    "\n",
    "with open(VAL_DATA_PATH, 'r') as file:\n",
    "    for _, line in enumerate(file):\n",
    "        val_data = pd.concat([\n",
    "            pd.DataFrame(\n",
    "                [line.replace('\\n', '').split(';')], columns=val_data.columns\n",
    "            ),\n",
    "            val_data\n",
    "        ], ignore_index=True)\n",
    "\n",
    "train_data = train_data.sample(frac=1, random_state=42)\n",
    "\n",
    "train_sentences = train_data['sentence']\n",
    "test_sentences = test_data['sentence']\n",
    "val_sentences = test_data['sentence']\n",
    "\n",
    "train_labels = train_data['label'].replace(\n",
    "    to_replace=['sadness', 'surprise', 'fear', 'anger', 'joy', 'love'], \n",
    "    value=[0, 1, 2, 3, 4, 5]\n",
    ")\n",
    "test_labels = test_data['label'].replace(\n",
    "    to_replace=['sadness', 'surprise', 'fear', 'anger', 'joy', 'love'], \n",
    "    value=[0, 1, 2, 3, 4, 5]\n",
    ")\n",
    "val_labels = test_data['label'].replace(\n",
    "    to_replace=['sadness', 'surprise', 'fear', 'anger', 'joy', 'love'], \n",
    "    value=[0, 1, 2, 3, 4, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'Length of Training Set: { len(train_data) }')\n",
    "print(f'Length of Test Set: { len(test_data) }')\n",
    "print(f'Length of Val Set: { len(val_data) }')\n",
    "\n",
    "print('')\n",
    "\n",
    "print(f'Number of Labels: { len(train_data[\"label\"].unique()) }')\n",
    "print('Possible Labels:')\n",
    "for label in train_data['label'].unique():\n",
    "    print(f'    { label }')\n",
    "    print(f'        Count: { len(train_data[train_data[\"label\"] == label]) }')\n",
    "    print(f'        % of Total: { len(train_data[train_data[\"label\"] == label]) / len(train_data) * 100.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def view_random_sentence():\n",
    "    row_num = random.randint(0, len(train_data) - 1)\n",
    "\n",
    "    print(f'emotion: { train_data.iloc[row_num][\"label\"] }')\n",
    "    print(f'sentence: { train_data.iloc[row_num][\"sentence\"] }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "view_random_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=12000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=15\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'''\n",
    "    Total number of words: { len(text_vectorizer.get_vocabulary()) }\n",
    "    Top 10 most common words: { text_vectorizer.get_vocabulary()[:10] }\n",
    "    Top 10 least common words: { text_vectorizer.get_vocabulary()[-10:] }      \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_1 = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=128,\n",
    "    embeddings_initializer='uniform',\n",
    "    input_length=15,\n",
    "    name='embedding'\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding_1(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='model_1_simple_dense')\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_1_history = model_1.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_1.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_2 = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=128,\n",
    "    embeddings_initializer='uniform',\n",
    "    input_length=15,\n",
    "    name='embedding_2'\n",
    ") \n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding_2(x)\n",
    "x = tf.keras.layers.LSTM(64)(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs, name='model_2_simple_lstm')\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_2_history = model_2.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_2.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_3 = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=128,\n",
    "    embeddings_initializer='uniform',\n",
    "    input_length=15,\n",
    "    name='embedding_3'\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding_3(x)\n",
    "x = tf.keras.layers.GRU(64, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs, name='model_2_simple_gru')\n",
    "\n",
    "model_3.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_3 = model_3.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_3.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_4 = tf.keras.layers.Embedding(\n",
    "    input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "    output_dim=128,\n",
    "    embeddings_initializer='uniform',\n",
    "    input_length=15,\n",
    "    name='embedding_4'\n",
    ")\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding_4(x)\n",
    "x = tf.keras.layers.Conv1D(\n",
    "    filters=32, \n",
    "    kernel_size=5, \n",
    "    activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    ")(x)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='model_4_simple_cnn')\n",
    "\n",
    "model_4.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_4_history = model_4.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_4.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12, 7))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.plot(model_1_history.history['accuracy'], label='Train')\n",
    "plt.plot(model_1_history.history['val_accuracy'], label='Validation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Dense')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "\n",
    "plt.plot(model_2_history.history['accuracy'], label='Train')\n",
    "plt.plot(model_2_history.history['val_accuracy'], label='Validation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('LSTM')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.plot(history_3.history['accuracy'], label='Train')\n",
    "plt.plot(history_3.history['val_accuracy'], label='Validation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('GRU')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "\n",
    "plt.plot(history_3.history['accuracy'], label='Train')\n",
    "plt.plot(history_3.history['val_accuracy'], label='Validation')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('CNN')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:31:58.561533Z",
     "iopub.status.busy": "2025-05-01T19:31:58.560888Z",
     "iopub.status.idle": "2025-05-01T19:31:58.565942Z",
     "shell.execute_reply": "2025-05-01T19:31:58.565007Z",
     "shell.execute_reply.started": "2025-05-01T19:31:58.561500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import pickle\n",
    "import shutil\n",
    "import os\n",
    "import json \n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:01.551771Z",
     "iopub.status.busy": "2025-05-01T19:30:01.551461Z",
     "iopub.status.idle": "2025-05-01T19:30:01.560796Z",
     "shell.execute_reply": "2025-05-01T19:30:01.559818Z",
     "shell.execute_reply.started": "2025-05-01T19:30:01.551742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '/kaggle/input/emotions-dataset-for-nlp/train.txt'\n",
    "TEST_DATA_PATH  = '/kaggle/input/emotions-dataset-for-nlp/test.txt'\n",
    "VAL_DATA_PATH   = '/kaggle/input/emotions-dataset-for-nlp/val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:01.565247Z",
     "iopub.status.busy": "2025-05-01T19:30:01.564952Z",
     "iopub.status.idle": "2025-05-01T19:30:01.574567Z",
     "shell.execute_reply": "2025-05-01T19:30:01.573675Z",
     "shell.execute_reply.started": "2025-05-01T19:30:01.565219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    records = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            sentence, label = line.strip().split(';')\n",
    "            records.append({'sentence': sentence, 'label': label})\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:02.507680Z",
     "iopub.status.busy": "2025-05-01T19:30:02.507298Z",
     "iopub.status.idle": "2025-05-01T19:30:02.556162Z",
     "shell.execute_reply": "2025-05-01T19:30:02.555522Z",
     "shell.execute_reply.started": "2025-05-01T19:30:02.507652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = load_data(TRAIN_DATA_PATH).sample(frac=1, random_state=42)\n",
    "test_df  = load_data(TEST_DATA_PATH)\n",
    "val_df   = load_data(VAL_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:03.840312Z",
     "iopub.status.busy": "2025-05-01T19:30:03.839985Z",
     "iopub.status.idle": "2025-05-01T19:30:03.854827Z",
     "shell.execute_reply": "2025-05-01T19:30:03.853849Z",
     "shell.execute_reply.started": "2025-05-01T19:30:03.840281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['label'] != 'love'].sample(frac=1, random_state=42)\n",
    "val_df   = val_df[val_df['label'] != 'love']\n",
    "test_df  = test_df[test_df['label'] != 'love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:04.983176Z",
     "iopub.status.busy": "2025-05-01T19:30:04.982495Z",
     "iopub.status.idle": "2025-05-01T19:30:04.987250Z",
     "shell.execute_reply": "2025-05-01T19:30:04.986433Z",
     "shell.execute_reply.started": "2025-05-01T19:30:04.983146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_sentences = train_df['sentence']\n",
    "val_sentences   = val_df['sentence']\n",
    "test_sentences  = test_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:07.131373Z",
     "iopub.status.busy": "2025-05-01T19:30:07.131015Z",
     "iopub.status.idle": "2025-05-01T19:30:07.141884Z",
     "shell.execute_reply": "2025-05-01T19:30:07.140985Z",
     "shell.execute_reply.started": "2025-05-01T19:30:07.131322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_map = {'sadness':0, 'surprise':1, 'fear':2, 'anger':3, 'joy':4}\n",
    "train_labels = train_df['label'].map(label_map)\n",
    "val_labels   = val_df['label'].map(label_map)\n",
    "test_labels  = test_df['label'].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:34.349864Z",
     "iopub.status.busy": "2025-05-01T19:30:34.349092Z",
     "iopub.status.idle": "2025-05-01T19:30:34.354300Z",
     "shell.execute_reply": "2025-05-01T19:30:34.353408Z",
     "shell.execute_reply.started": "2025-05-01T19:30:34.349829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:31:00.668434Z",
     "iopub.status.busy": "2025-05-01T19:31:00.668078Z",
     "iopub.status.idle": "2025-05-01T19:31:00.899085Z",
     "shell.execute_reply": "2025-05-01T19:31:00.898105Z",
     "shell.execute_reply.started": "2025-05-01T19:31:00.668403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Применим предобработку\n",
    "train_df['sentence'] = train_df['sentence'].apply(preprocess_text)\n",
    "val_df['sentence']   = val_df['sentence'].apply(preprocess_text)\n",
    "test_df['sentence']  = test_df['sentence'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:30:11.611240Z",
     "iopub.status.busy": "2025-05-01T19:30:11.610594Z",
     "iopub.status.idle": "2025-05-01T19:30:13.523062Z",
     "shell.execute_reply": "2025-05-01T19:30:13.522367Z",
     "shell.execute_reply.started": "2025-05-01T19:30:11.611207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Предобработка: TextVectorization\n",
    "MAX_TOKENS = 12000\n",
    "SEQ_LEN    = 20\n",
    "EMBED_DIM  = 256\n",
    "\n",
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LEN\n",
    ")\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:32:01.412313Z",
     "iopub.status.busy": "2025-05-01T19:32:01.411688Z",
     "iopub.status.idle": "2025-05-01T19:32:01.417625Z",
     "shell.execute_reply": "2025-05-01T19:32:01.416782Z",
     "shell.execute_reply.started": "2025-05-01T19:32:01.412281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2)\n",
    "\n",
    "def save_model_and_vectorizer(model, model_name, vectorizer):\n",
    "    model.save(f\"{model_name}\", save_format='tf')\n",
    "\n",
    "    vectorizer_config = vectorizer.get_config()\n",
    "    vectorizer_weights = vectorizer.get_weights()\n",
    "\n",
    "    os.makedirs(f\"{model_name}_vectorizer\", exist_ok=True)\n",
    "\n",
    "    with open(f\"{model_name}_vectorizer/config.json\", \"w\") as f:\n",
    "        json.dump(vectorizer_config, f)\n",
    "\n",
    "    with open(f\"{model_name}_vectorizer/weights.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectorizer_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:32:03.615737Z",
     "iopub.status.busy": "2025-05-01T19:32:03.615073Z",
     "iopub.status.idle": "2025-05-01T19:33:05.232446Z",
     "shell.execute_reply": "2025-05-01T19:33:05.231622Z",
     "shell.execute_reply.started": "2025-05-01T19:32:03.615700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "460/460 [==============================] - 30s 49ms/step - loss: 0.9652 - accuracy: 0.6324 - val_loss: 0.4701 - val_accuracy: 0.8211 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.3553 - accuracy: 0.8783 - val_loss: 0.3284 - val_accuracy: 0.8743 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.2228 - accuracy: 0.9200 - val_loss: 0.3122 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1536 - accuracy: 0.9458 - val_loss: 0.4013 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.1108 - accuracy: 0.9598 - val_loss: 0.4176 - val_accuracy: 0.8716 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.0570 - accuracy: 0.9798 - val_loss: 0.5088 - val_accuracy: 0.8743 - lr: 3.0000e-04\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3381398618221283, 0.8777838349342346]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(len(text_vectorizer.get_vocabulary()), EMBED_DIM)(x)\n",
    "x = layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.3))(x)\n",
    "x = layers.Bidirectional(layers.GRU(64, dropout=0.3))(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(6, activation='softmax')(x)\n",
    "model_bigru = models.Model(inputs, outputs, name='model_bigru')\n",
    "model_bigru.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_bigru.fit(train_sentences, train_labels, epochs=10, validation_data=(val_sentences, val_labels), callbacks=[early_stop, reduce_lr])\n",
    "model_bigru.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T19:39:18.338876Z",
     "iopub.status.busy": "2025-05-01T19:39:18.338500Z",
     "iopub.status.idle": "2025-05-01T19:39:18.672262Z",
     "shell.execute_reply": "2025-05-01T19:39:18.671287Z",
     "shell.execute_reply.started": "2025-05-01T19:39:18.338847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 0s 4ms/step\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.97      0.95      0.96       581\n",
      "    surprise       0.95      0.91      0.93        66\n",
      "        fear       0.96      0.94      0.95       224\n",
      "       anger       0.94      0.95      0.94       275\n",
      "         joy       0.96      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.96      1841\n",
      "   macro avg       0.96      0.94      0.95      1841\n",
      "weighted avg       0.96      0.96      0.96      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_probs = model_bigru.predict(test_sentences)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(test_labels, y_pred, target_names=['sadness', 'surprise', 'fear', 'anger', 'joy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_model_and_vectorizer(model_bigru, 'model_bigru', text_vectorizer)\n",
    "\n",
    "shutil.make_archive('/kaggle/working/model_bigru_all', 'zip', '/kaggle/working/model_bigru')\n",
    "shutil.make_archive('/kaggle/working/vectorizer', 'zip', '/kaggle/working/model_bigru_vectorizer')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 605165,
     "sourceId": 1085454,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30513,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
